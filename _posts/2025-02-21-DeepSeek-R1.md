---
title: 'DeepSeek-R1技术报告解读与强化学习回顾'
date: 2025-02-21
permalink: /posts/2025/02/DeepSeek-R1/
tags:
  - DeepSeek
  - LLM
  - Reasoning
---

参考资料：
>[DeepSeek-R1技术路线图](https://www.reddit.com/r/LocalLLaMA/comments/1i66j4f/deepseekr1_training_pipeline_visualized/?rdt=64269)
>[PPO与GRPO，Jimmy Shi的博客](https://yugeten.github.io/posts/2025/01/ppogrpo/)
>[DeepSeek R1 理论概述 | GRPO + RL + SFT](https://www.youtube.com/watch?v=QdEuh2UVbu0)
>[DeepSeek群体相对策略优化（GRPO）-公式与代码](https://www.youtube.com/watch?v=Yi1UCrAsf4o)
>[蘑菇书EasyRL](https://datawhalechina.github.io/easy-rl/#/?id=%e8%98%91%e8%8f%87%e4%b9%a6easyrl)

---

# 0 关于RL的一些回顾
>RL的出现远早于LLM，为了将LLM中基于强化学习的训练方法统一到RL的框架下，你可能会接触到一些诸如奖励模型、评价模型、环境等似乎与LLM无关的名称。在这一部分，我们将讲解各个名词的含意。

## 0.1 RLHF
我们将RLHF的工作流程分为步骤：

1. 对于每个提示，请从模型中采样多个响应；
2. 人类按质量对这些产出进行排名；
3. 鉴于任何模型响应，**训练奖励模型**来预测人类的偏好 /排名；
4. 使用**RL（例如PPO，GRPO）** 微调模型以最大化奖励模型的分数。

正如我们可以看到的那样，这里的过程相对简单，具有两个可学习的部分，即奖励模型和RL。在这一部分，我们可以顺便回顾奖励模型的相关内容。

## 0.2 Reward Model
我们不能让人类对模型的所有输出进行排名。一种节省成本的方法是让注释者对LLM输出的一小部分进行评分，然后训练模型来预测这些注释者的偏好，这就是奖励模型的来源。

假设我们的奖励模型是$R_\phi$。给定一个prompt $p$，LLM将会生成 $N$ 个回复$r_i,...,r_N$。对于RLHF方法，人类将会对模型的输出进行排序，例如从 $r_i$ 到 $r_j$ （从高到低）。那么很自然的，为了训练一个模仿人类给模型打分的模型，我们会想到使用一个差值的对数函数来最小化损失函数训练模型：
$$
\mathcal{L}(\phi) = - \log \sigma(R_\phi(p, r_i) - R_\phi(p, r_j))
$$
 这一符合直觉的损失函数源自于**Bradley-Terry model**，此处不会展开讲解。只需要知道，奖励模型是一个用来给LLM的输出进行打分的模型，LLM的输出越符合约束，该条输出的奖励分数越高。
 
 请注意，部分响应的奖励始终是0；仅对于LLM的完整响应，奖励模型才能返回非零标量分数。这一重要事实将在稍后变得重要。

---

# 1 关于PPO
> 这一部分仅仅只是为了粗略地讲解PPO，这样你可能会对GRPO有更深刻的理解，但直接进入GRPO部分也不会造成任何理解上的困难。

首先，一个抽象的概念是，PPO代表近端优化策略，它由以下部分组成：

1. 策略模型：一个经过Pretrain或SFT后的模型
2. 奖励模型：经过训练的冷冻网络，可以完全回应提示，从而提供标量奖励；
3. 评价模型：也称为价值函数，这是一个可学习的网络，可以**部分响应**提示并预测标量奖励，通常来说，评价模型的大小将与策略模型一致。

这三个组件将经过以下五个步骤完成一次RL循环：

1. 生成响应：LLM（策略模型）为给定提示产生多个响应；
2. 分数响应：奖励模型为每个响应分配奖励；
3. 计算优势：使用GAE计算优势（稍后再详细介绍，它用于训练LLM）；
4. 优化策略：通过优化总目标来更新LLM（策略模型）；
5. 更新评价模型：训练价值功能可以更好地预测给定**部分响应**的奖励。

你可能会奇怪部分响应的含意，在后文我们会引出这一部分的解释。现在，让我们在更多细节中查看其中的一些阶段/组件，然后看看它们如何融合在一起。

## 1.1 States and Actions
在开始之前，我们仍旧需要明确一些术语，这些术语来源于RL，我们希望将整个训练过程统一在RL的框架下：

1. States：状态，套用到LLM中表示当前模型的输出，记作 $s_t$ ；
2. Actions：动作，套用到LLM表示模型在当前状态下输出的下一个token，记作 $a_t$ 。

其中，$t$ 在RL中表示时间步，但在当前表示的是token-level。一个简单的例子，对LLM给定一个prompt $p$，LLM将会生成一个长度为 $T$ 的回答 $r_i$ 。

- $t=0$ 时，我们的状态就是prompt，也就是 $s_0=p$ ，而 $a_0$ 表示LLM生成的第一个token；
- $t=1$ 时，$s_1=p,a_0$，LLM将会生成下一个token，也就是 $a_1$；
- $t=T-1$ 时，$s_{T-1}=p,a_{0:T-2}$，LLM将会生成下一个token，也就是$a_{T-1}$。

>注意，此处的 $r_i$ 是instance-level而不是token-level。我们把所有actions连接起来就等价于 $r_i$ ，也就是 $r_i = a_0, a_1,...,a_{T-1}$

## 1.2 General Advantage Estimation (GAE)
我们的策略模型（LLM）需要更新以得到更好的Advantage（优势）。从直觉上说，它定义了当前状态 $s_t$ 下做出的action $a_t$ 相比于一个平均的action“好”了多少。定义如下：
$$
A_t=Q(s_t,a_t)-V(s_t)
$$
此处，$Q(s_t,a_t)$ 是在状态 $s_t$ 下采取特定行动 $a_t$ 的**预期累积奖励**。$V(s_t)$ 是策略模型在状态st下采取的**平均行动**的预期累积奖励。

有两种主要方式来估计这种优势，每种方式都有其权衡，具体如下：

1. **蒙特卡洛方法（Monte-Carlo, MC）**：使用整个轨迹的奖励（即完整的响应）。这种方法由于稀疏奖励的原因，具有较高的方差——从大语言模型（LLM）中采集足够的样本来使用MC优化是很昂贵的，但它的偏差较低，因为我们可以准确地建模奖励。
2. **时间差分方法（Temporal Difference, TD）**：使用一步轨迹奖励（即衡量给定提示下刚生成的词的质量）。通过这种方式，我们可以在token级别计算奖励，这显著减少了方差，但同时，偏差上升，因为我们无法像在完整生成的响应中那样准确预测最终的奖励。

在这里，广义优势估计（GAE）就发挥了作用——它通过多步TD来平衡偏差和方差。然而，回想一下我们之前提到过，如果响应不完整，奖励模型会返回0：在不知道生成某个词前后奖励如何变化的情况下，我们怎么计算TD呢？因此，我们引入了一个专门做这件事的模型，我们称之为评价模型。

## 1.3 评价模型
评价模型被训练为在只有部分状态时预期最终奖励，这样我们就可以计算TD。训练评价模型$V$其实非常直接（直接估计期望），对于给定的部分状态 $s_t$ 我们想要预测在最终状态（$s_T=p,r$）下奖励模型的输出。
$$
L(\gamma) = \mathbb{E}_t \left[ \left( V_\gamma(s_t) - \text{sg}(R_\phi(s_T)) \right)^2 \right]
$$
sg表示停止梯度操作。如我们所见，评价模型接受了简单的L2损失对奖励模型的分数。

奖励模型 $R$ 在PPO和冷冻之前接受了训练，其工作也只是为了预测奖励。然而评价模型需要接受与LLM相同的训练。这是因为鉴于当前的策略模型，价值函数必须估算部分响应的奖励。因此，评价模型必须与策略模型一起进行更新，以避免其预测变得过时和未对准。这就是所谓的actor-critic。

现在，我们有一种方法可以预见部分状态的奖励。现在让我们继续使用GAE，如前所述，它计算了一个多步TD目标：
$$
A_{K}^{\text{GAE}} = \delta_0 + \lambda \delta_1 + \lambda^2 \delta_2 + \cdots + (\lambda)^{K-1} \delta_{K-1} = \sum_{t=0}^{K-1} (\lambda)^t \delta_t,
$$
其中，$K$表示TD的步数（$K<T$）。t表示时间步 $t$ 上的TD error，也就是：
$$
\delta_t = V_\gamma(s_{t+1}) - V_\gamma(s_t)
$$
简而言之，TD误差计算了一个时间步的预期总奖励与当前状态的价值函数之间的差异。GAE通过计算K个时间步内的聚合单步TD误差来估计优势。在GAE中，$\lambda$控制了方差和偏见之间的权衡：

- 当 $\lambda=0$，GAE退化成单步TD；
- 当 $\lambda=1$，GAE等价于MC。

>我在这里省略了一些细节。原本在GAE中还有一个折扣因子，但通常我们都会设置为1，因此在此处省略。这会极大的简化公式的复杂程度。

在RLHF中，我们希望最大化此优势，从而最大程度地提高LLM生成的每个token的奖励。

## 1.4 PPO Objective
终于，我们来到了最终的组装部分。在了解了以上内容以后，我们再来查看PPO的目标函数的组成：

1. 剪裁后的替代目标 clipped surrogate objective；
2. 熵奖励 Entropy bonus；
3. KL惩罚。

### 1.4.1 clipped surrogate objective
我们将在这一步最大化优势函数AGAEK，使得对每个LLM生成的token都可以预测最大的奖励（也就是说，根据优势的定义，LLM预测的每个令牌都应该比其平均预测好得多）。首先，我们写下需要优化的损失函数L，稍后我们会了解到==剪裁后的替代目标==的具体意义。
$$
L^{\text{clip}}(\theta) = \mathbb{E}_t \left[ \min \left( c_t(\pi_\theta) A_t^{GAE}, \text{clip}(c_t(\pi_\theta), 1 - \epsilon, 1 + \epsilon) A_t^{GAE} \right) \right],
$$
此处的 $\epsilon$ 控制剪裁的范围。$c_t(\pi_\theta)$ 是在给定累计状态st的情况下预测特定token at的概率比，这一比值由策略模型更新前、后构成。
$$
c_t(\pi_\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}.
$$
一个简单的例子来了解clip到底是如何工作的：

1. 假设某个token有如下概率分布
	- 在LLM未更新时的概率为0.1
	- 在LLM更新后的概率为0.3
2. 根据公式，我们计算得 $c_t = \frac{0.3}{0.1}=3$
3. 如果我们取 $\epsilon=0.2$，那么 $c_t$ 被剪裁后的值将为 $1.2$
4. 最终，clipped surrogate loss将会是 $L^{\text{clip}}(\theta) = 1.2A_t^{GAE}$。

你可以将剪裁视为防止过度自信的一种方式。如果没有剪裁，那么一个很大的$A$将会导致过度激进的更新策略。一个更直观的可视化如下：

![[Pasted image 20250220015216.png]]

### 1.4.2 KL divergence penalty
除了Clip，我们还有KL散度惩罚来限制模型不要偏离原始模型（Reference Model）太多。此处的原始模型通常指在优化开始前的模型。公式如下：
$$
\text{KL}(\theta) = \mathbb{E}_{s_t} \left[ \mathbb{D}_{\text{KL}} \left( \pi_{\theta_{\text{Ref}}}(\cdot | s_t) \, \middle| \middle| \, \pi_\theta(\cdot | s_t) \right) \right]
$$
```python
# Compute KL divergence between original and current policy/model
logits_orig = original_model(states)  # Original model's logits
logits_current = current_model(states)  # Current model's logits

probs_orig = F.softmax(logits_orig, dim=-1)
log_probs_orig = F.log_softmax(logits_orig, dim=-1)
log_probs_current = F.log_softmax(logits_current, dim=-1)

kl_div = (probs_orig * (log_probs_orig - log_probs_current)).sum(dim=-1)
kl_penalty = kl_div.mean()  # Average over sequence and batch
```

### 1.4.3 Entropy bonus
该损失通过惩罚低熵来鼓励LLM探索不同的生成。
$$
H(\theta) = - \mathbb{E}_{a_t} \left[ \log \pi_\theta(a_t | s_t) \right].
$$
```python
# Compute entropy of current policy
probs_current = F.softmax(logits_current, dim=-1)
log_probs_current = F.log_softmax(logits_current, dim=-1)

entropy = -(probs_current * log_probs_current).sum(dim=-1)
entropy_bonus = entropy.mean()  # Average over sequence and batch
```

### 1.4.4 Finally, the PPO objective
根据上面的三个函数，还有评价模型的MSE损失（在先前与策略函数一起进行了优化），PPO目标定义为如下：
$$
\mathcal{L}_{\text{PPO}}(\theta, \gamma) = \underbrace{\mathcal{L}_{\text{clip}}(\theta)}_{\text{maximise reward}} + \underbrace{w_1 H(\theta)}_{\text{Maximise entropy}} - \underbrace{w_2 \text{KL}(\theta)}_{\text{Penalise KL divergence}} - \underbrace{w_3 \mathcal{L}(\gamma)}_{\text{Critic L2}}
$$
最终，我们得到了完整的PPO优化函数，并遵从一开始讲到的5个步骤进行PPO优化。

---

# 2 GRPO
## 2.1 What's the different?
既然我们对PPO有很好的了解，那么非常容易理解GRPO，而关键区别在于两种算法如何估计优势$A$。不像在PPO中那样通过评价模型估算优势，而是通过使用相同的提示从LLM中获取多个样本来做到这一点。流程如下：

1. 对于每一个prompt $p$，从策略模型（LLM）中采样出一组（a Group）回答 $\mathcal{G} = r_1, r_2, \ldots, r_N$；
2. 对于每一个回答都计算一个奖励
3. 计算每个响应的群体标准化优势（group-normalized advantage）：
$$A_i = \frac{R_\phi(r_i) - \text{mean}(\mathcal{G})}{\text{std}(\mathcal{G})}$$ 
此处的$\text{mean}(\mathcal{G})$、$\text{std}(\mathcal{G})$分别表示组内平均值和标准偏差。

在GRPO中，优势近似为其在其一系列响应组中的每个响应的归一化奖励。这消除了评论网络计算每步奖励的需求。也就是说，在GRPO的优化过程中，我们不需要生成一个与策略模型同样大小的评价模型了。

## 2.2 GRPO objective
与PPO相似，GRPO仍然利用剪裁的替代损失以及KL惩罚。此处未使用Entropy bonus，因为基于组的抽样已经鼓励探索。剪裁的替代损失与PPO中使用的损失相同。完整的公式如下：
$$
\mathcal{L}_{\text{GRPO}}(\theta) = \underbrace{\mathcal{L}_{\text{clip}}(\theta)}_{\text{Maximise reward}} - \underbrace{w_1 \mathbb{D}_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})}_{\text{Penalise KL divergence}}
$$
此处，
$$
\mathcal{L}_{\text{clip}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left( \min \left( \frac{\pi_\theta(r_i | p)}{\pi_{\theta_{\text{old}}}(r_i | p)} A_i, \, \text{clip} \left( \frac{\pi_\theta(r_i | p)}{\pi_{\theta_{\text{old}}}(r_i | p)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) \right)
$$
![[Pasted image 20250219223355.png]]
## 2.3 关于PPO与GRPO的总结
![[Pasted image 20250220022836.png]]

---

# 3 DeepSeek-R1
## 3.1 TLDR
三个主要贡献与模型：

- DeepSeek-R1-Zero：不用SFT直接进行RL，也能取得不错的效果。
- DeepSeek-R1 ：加入少量（数千量级）CoT数据进行SFT作为冷启动，然后再进行RL，可以取得更优的性能。同时回答更符合人类偏好。
- 用DeepSeek-R1的样例去蒸馏小模型，能取得惊人的效果。

与现有的Reasoning Model的对比：
![[Pasted image 20250220130238.png]]

## 3.2 DeepSeek-R1-Zero
该模型的训练过程极其简单：
- 使用DeepSeek-V3做为Base Model训练；
- 使用GRPO做为RL训练中的算子；
	- 使用一个“平平无奇”的模板；
	- Reward Model：
		- **准确性奖励**：对于数学问题，直接匹配标准答案；对于代码问题，基于编译执行单测去验证。
		- **格式奖励**：看CoT过程是否以标准\<think> \</think>包裹。
![[Pasted image 20250220130326.png]]
 
 从方法层面上来看，简单暴力，但是效果出奇的好。看起来随着训练步数的增加，性能稳步提升，达到和OpenAI-o1-0912接近的水平。
 ![[Pasted image 20250220130928.png]]
 
 并且观察到了明显的“进化”现象，随着训练步数的增加，输出平均长度也在增加。意味着LLM似乎自己已经潜移默化学会了进行更多的思考和推理，达到更好的效果。
 ![[Pasted image 20250220131100.png]]
 
 甚至出现了自主的“Aha Moment”情况，突然就能开始反思。
![[Pasted image 20250220131155.png]]

然而完全的RL路线也不是那么完美，DeepSeek团队也发现了一些问题，比如，DeepSeek-R1-Zero 生成的答案可读性相对差、存在混合语言输出情况。为了让模型说人话，还是得加点SFT。这就是DeepSeek-R1的部分了。

## 3.3 DeepSeek-R1
DeepSeek-R1-Zero已经证明了，完全不进行SFT, 直接RL就能显著提升LLM的推理能力；但是同时输出的可读性、混合语言输出问题是很大的问题。可别忘了SFT不就是为了遵循指令，让LLM模仿说人话吗？那把SFT阶段再加上不就得了。既然完全不SFT也能有非常好的效果，那少加一点是不是就能让LLM学会说人话了，同时推理能力也能进一步提升呢？DeepSeek-R1采用如下4个阶段，又把能力进一步加强。

- **少量Long CoT数据的冷启动**：大约数千条（thousands of）冷启动数据，对于长CoT数据，使用few-shot，直接提示DeepSeek-R1-Zero通过反思和验证生成详细答案，然后通过人工注释者的后处理来细化结果。（具体的数据和处理方法在文中未提及，我们无法知道到底有多少数据，也不知道具体如何处理）
- **推理阶段的RL和新的奖励机制**：对数学、代码等推理场景进行RL。和DeepSeek-R1-Zero一样的方式。针对DeepSeek-R1-Zero输出中语言混合的情况，额外增加一个奖励：==语言一致性奖励==，统计输出中目标语言的占比作为奖励信号。将原始的准确性奖励与语言一致性奖励求和作为最终奖励，进行过程反馈。
- **拒绝采样和SFT**：这一步主要是为了提升模型的==通用能力==，通过构建两部分的数据进行SFT来实现。
	1. 推理数据：采用拒绝采样的方式从前一阶段得到的模型生成推理过程，同时额外引入一些无法用规则进行奖励的数据（这部分数据使用DeepSeek-V3通过LLM-as-judge的方式进行奖励，比较GroudTruth与实际输出）。同时，过滤掉了包含混合语言、长段落、代码块的CoT数据。总计有600k样本。
	2. 非推理数据：使用DeepSeek-V3生成、使用DeepSeek-V3的SFT数据，共计200k推理无关的样本。
	3. 这一阶段总共生成了80w样本，用DeepSeek-V3-Base 进行了2个epoch的SFT。
- **适配所有场景的RL阶段**：最后为了同时平衡推理能力和通用能力，又进行了一次RL。对于不同的数据类型，采用不同的prompt和奖励。
	- 对于推理数据，使用DeepSeek-R1-Zero中的方法，该方法在数学、编程和逻辑推理领域使用基于规则的奖励指导学习过程。
	- 对于通用数据，使用通用的RM来进行奖励。基本复用DeepSeek-V3的方式。
	- 对于有用性，专注于评估最终的summary，确保评估对用户的实用性和相关性，同时尽量减少对底层推理过程的干扰。
	- 对于无害性，评估模型的整个响应，包括推理过程和总结，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容。

最终，奖励信号和多样化数据分布的整合使得最终的模型既能保持推理能力，又能满足有用性和无害性，取得比较好的用户体验。
![[Pasted image 20250220132541.png]]

## 3.4 蒸馏小模型
直接用DeepSeek-R1阶段三：“拒绝采样和SFT” 时的数据对小模型进行SFT，**不包含RL阶段**，就能取得比较好的效果。
![[Pasted image 20250220132558.png]]
DeepSeek在Qwen和LLaMA的多个小模型上进行了尝试，效果依然非常显著。（不用自家小模型的原因可能是V2-Lite实在太烂了）

为什么不在小模型上尝试SFT+RL的操作？从实验结果来看，蒸馏效果更好。用小的模型一顿SFT+RL操作，最后的效果还远不如直接蒸馏更好性能模型的输出直接SFT。
![[Pasted image 20250303150323.png]]

## 3.5 总结
 一张图总结讲解DeepSeek-R1技术路线：
![[Pasted image 20250219201938.png]]

```json
{ 
	"messages": [ 
		{ "role": "system", "content": "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>" 
		}, 
		{ "role": "user", "content": "Please reason step by step, and put your final answer within \boxed{}. Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?" 
		} 
	], 
	"solution": "\boxed{72}" 
}
```

格式奖励函数：
```python
class Format(ORM): 
	def __call__(self, completions, **kwargs) -> List[float]: 
		"""
		Reward function that checks if the completion has a specific format.
		"""
		pattern = r'^<think>.*?</think>\s*<answer>.*?</answer>$'
		matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completions]
	
		return [1.0 if match else 0.0 for match in matches]

```

准确性奖励函数：
```python
class MathAccuracy(ORM):

    def __init__(self):
        import importlib.util
        assert importlib.util.find_spec('math_verify') is not None, (
            "The math_verify package is required but not installed. Please install it using 'pip install math_verify'.")

    def __call__(self, completions, solution, **kwargs) -> List[float]:
        from latex2sympy2_extended import NormalizationConfig
        from math_verify import LatexExtractionConfig, parse, verify
        rewards = []
        for content, sol in zip(completions, solution):
            gold_parsed = parse(sol, extraction_mode='first_match', extraction_config=[LatexExtractionConfig()])
            if len(gold_parsed) != 0:
                # We require the answer to be provided in correct latex (no malformed operators)
                answer_parsed = parse(
                    content,
                    extraction_config=[
                        LatexExtractionConfig(
                            normalization_config=NormalizationConfig(
                                nits=False,
                                malformed_operators=False,
                                basic_latex=True,
                                equations=True,
                                boxed=True,
                                units=True,
                            ),
                            # Ensures that boxed is tried first
                            boxed_match_priority=0,
                            try_extract_without_anchor=False,
                        )
                    ],
                    extraction_mode='first_match',
                )
                # Reward 1 if the content is the same as the ground truth, 0 otherwise
                reward = float(verify(answer_parsed, gold_parsed))
            else:
                # If the gold solution is not parseable, we reward 1 to skip this example
                reward = 1.0
            rewards.append(reward)
        return rewards
```




