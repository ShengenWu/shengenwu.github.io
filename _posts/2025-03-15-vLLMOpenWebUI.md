---
title: 'vLLM+OpenWebUIéƒ¨ç½²å¤§æ¨¡å‹ç½‘é¡µé—®ç­”ç®€æ˜“æŒ‡å—'
date: 2025-03-15
permalink: /posts/2025/03/vLLMOpenWebUI/
tags:
  - æœ¬åœ°éƒ¨ç½²
  - LLM
---

## ä¸‹è½½æƒé‡æ–‡ä»¶

> æ¥æºï¼š[å¦‚ä½•å¿«é€Ÿä¸‹è½½huggingfaceæ¨¡å‹â€”â€”å…¨æ–¹æ³•æ€»ç»“](https://zhuanlan.zhihu.com/p/663712983)

ç”±äºä¼—æ‰€å‘¨çŸ¥çš„åŸå› ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è¿æ¥Hugging faceä¸‹è½½ã€‚å› æ­¤ï¼Œé™¤äº†ä½¿ç”¨æ¢¯å­ä»¥å¤–ï¼Œæ›´æ¨èä½¿ç”¨é•œåƒæ¥è¿›è¡Œä¸‹è½½ï¼Œåˆå¿«åˆå¥½ï¼š
```bash
export HF_ENDPOINT='https://hf-mirror.com'
```
ç„¶åæˆ‘ä»¬ä¸‹è½½Hugging faceå®˜æ–¹çš„ä¸‹è½½å·¥å…·é“¾`huggingface-cli`ï¼š
```bash
pip install -U huggingface_hub
```
æ¥ä¸‹æ¥å¼€å§‹å°†æ¨¡å‹æƒé‡ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„ï¼š
```bash
cd /data/share
huggingface-cli download Qwen/QwQ-32B --local-dir QwQ-32B
```

---
## éƒ¨ç½²vLLM

> æ¥æºï¼š[vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/en/stable/index.html)

æœ€å¥½æ–°å»ºä¸€ä¸ªå…¨æ–°çš„`conda`ç¯å¢ƒä»¥é¿å…å¯èƒ½çš„é”™è¯¯ï¼š
```bash
conda create -n vllm python=3.12 -y
conda activate vllm
```

å®˜æ–¹æ¨èä½¿ç”¨`uv`è¿›è¡Œç¯å¢ƒç®¡ç†ï¼Œå¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://docs.vllm.ai/en/stable/getting_started/installation/gpu/index.html)ã€‚ä½†åœ¨æ­¤å¤„ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„`conda`+`pip`æ¥ç®¡ç†ç¯å¢ƒï¼š
```bash
pip install vllm
```

æ¥ä¸‹æ¥ç›´æ¥å¯åŠ¨vllmï¼š
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve /data/share/QwQ-32B \
Â  --enable-reasoning --reasoning-parser deepseek_r1 \
Â  --served-model-name QwQ-32B \
Â  --tool-call-parser hermes \
Â  --max-model-len=68864 \
Â  --tensor-parallel-size 4 \
Â  --port 8000
```
å‚æ•°è§£è¯»ï¼š
- `enable-reasoning`å’Œ`--reasoning-parser`è¡¨ç¤ºå°†QwQçš„è¾“å‡ºæ ¼å¼åŒ–ä¸º`deepseek_r1`çš„å½¢å¼ï¼Œè¿™åœ¨åç»­ä½¿ç”¨Open WebUIå¤„ç†æ€è€ƒè¿‡ç¨‹æ—¶å°†å˜å¾—æœ‰ç”¨ã€‚
- `served-model-name`æ˜¯è°ƒç”¨è¯¥APIæ—¶ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚
- `tensor-parallel-size`æŒ‡å®šæ¨¡å‹å‚æ•°å¹¶è¡Œåœ¨å•èŠ‚ç‚¹`4`å¡GPUä¸Šã€‚
- `tool-call-parser`ç”¨äºæŒ‡å®šå¦‚ä½•è§£æå’Œå¤„ç†å·¥å…·è°ƒç”¨ã€‚å®ƒå®šä¹‰äº†ï¼š
	- å¦‚ä½•è§£æå·¥å…·è°ƒç”¨çš„è¾“å…¥ï¼šå°†æ¨¡å‹çš„è¾“å‡ºè§£æä¸ºå·¥å…·è°ƒç”¨çš„ç»“æ„åŒ–æ•°æ®ã€‚
	- å¦‚ä½•å¤„ç†å·¥å…·è°ƒç”¨çš„ç»“æœï¼šå°†å·¥å…·è°ƒç”¨çš„ç»“æœè¿”å›ç»™æ¨¡å‹ï¼Œä»¥ä¾¿æ¨¡å‹ç»§ç»­ç”Ÿæˆåç»­å†…å®¹ã€‚
	- `hermes` è¡¨ç¤ºä½¿ç”¨Hermesè§£æå™¨æ¥å¤„ç†å·¥å…·è°ƒç”¨ã€‚
- `max-model-len`ç”¨äºæŒ‡å®šä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå—`KVCache`å½±å“ï¼Œä¸Šä¸‹æ–‡é•¿åº¦æœ‰æœ€å¤§å€¼ï¼Œè¶…å‡ºåˆ™æ— æ³•æ­£å¸¸å¯åŠ¨æœåŠ¡ã€‚
- `port`ç”¨äºæŒ‡å®šæœåŠ¡å¯åŠ¨åœ¨å“ªä¸ªç«¯å£ä¸Šã€‚
---

## éƒ¨ç½²OpenWeb UI

> æ¥æºï¼š[OpenWeb UIå®˜æ–¹GitHub](https://github.com/open-webui/open-webui)

### éƒ¨ç½²å¹¶è¿è¡ŒOpen WebUIæœ¬ä½“
è¿™ä¸€æ­¥éå¸¸ç®€å•ï¼Œåªéœ€è¦é…ç½®å¥½ç¯å¢ƒå¹¶ä¸€é”®è¿è¡Œå³å¯ï¼š
```bash
conda activate vllm
pip install open-webui
open-webui serve
```

`open-webui`é»˜è®¤å¼€å¯åœ¨`8080`ç«¯å£ä¸Šã€‚

### é…ç½®Open WebUI
åœ¨å¼€å§‹ä¹‹å‰ç¡®ä¿`vLLM`æœåŠ¡å·²å¼€å¯åœ¨`8000`ç«¯å£ä¸Šï¼š
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve /data/share/QwQ-32B \
Â  --enable-reasoning --reasoning-parser deepseek_r1 \
Â  --served-model-name QwQ-32B \
Â  --tool-call-parser hermes \
Â  --max-model-len=68864 \
Â  --tensor-parallel-size 4 \
Â  --port 8000
```

ç„¶åè®¿é—®`http://YourLocalHostIP:8080/`ï¼Œåœ¨åˆæ¬¡å¼€å¯æœåŠ¡æ—¶éœ€è¦æ–°å»ºç®¡ç†å‘˜è´¦æˆ·ï¼Œç™»å½•åä¸ºé‚®ç®±ï¼Œåç§°ä¸ºæ˜¾ç¤ºåœ¨æ§åˆ¶å°ä¸Šçš„åå­—ï¼Œé‚®ç®±åªç”¨åšç™»å½•åæ— é¡»éªŒè¯ã€‚

æ¥ä¸‹æ¥ï¼Œç‚¹å‡»å·¦ä¸‹è§’å¤´åƒï¼Œè¿›å…¥`ç®¡ç†å‘˜é¢æ¿`ï¼Œæ‰¾åˆ°`è®¾ç½®`ã€‚

- ç‚¹å‡»`å¤–éƒ¨é“¾æ¥`
- ç‚¹å‡»`ç®¡ç†OpenAI APIè¿æ¥`æ—è¾¹çš„`+`åŠ å·

æŒ‰ç…§å›¾ç¤ºæ“ä½œå®ŒæˆAPIç«¯å£çš„é…ç½®ã€‚
![[Pasted image 20250308142936.png]]

ä¿å­˜åä½ å°†å¯ä»¥çœ‹åˆ°åœ¨`æ¨¡å‹`é€‰é¡¹å¡å‡ºç°`QwQ-32B`çš„é€‰é¡¹ã€‚
![[Pasted image 20250308143353.png]]

å¯ç”¨è¯¥æ¨¡å‹åï¼Œå·²ç»å¯ä»¥åœ¨å¯¹è¯ä¸­è°ƒç”¨è¯¥æ¨¡å‹äº†ã€‚ä½†æ˜¯ï¼Œä½ ä¼šå‘ç°æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ²¡æœ‰å¾ˆä¼˜é›…çš„å±•ç¤ºå‡ºæ¥ï¼Œè€Œä¸”æ— æ³•æ˜¾ç¤ºè¾“å…¥è¾“å‡ºçš„tokenæ•°é‡å’Œè¾“å‡ºé€Ÿåº¦ï¼Œæ‰€ä»¥éœ€è¦å¯¹`å‡½æ•°`é€‰é¡¹å¡è¿›è¡Œé…ç½®ã€‚

### é…ç½®å‡½æ•°
è½¬åˆ°`å‡½æ•°`é€‰é¡¹å¡ï¼Œå¹¶æ–°å»ºä¸€ä¸ªå‡½æ•°ï¼Œå¡«å…¥å‡½æ•°åç§°ã€IDå’Œæè¿°ï¼ˆæ³¨æ„ä¸è¦åŒ…å«ä»»ä½•ç‰¹æ®Šå­—ç¬¦ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼‰ï¼š
```python
"""
title: qwq-32b-preview Output Beautifier
author: nitefood
author_url: https://github.com/nitefood
version: 0.1
"""

from pydantic import BaseModel
from typing import Optional
import re


class Filter:

    async def outlet(self, body: dict, user: Optional[dict] = None) -> dict:
        messages = body.get("messages", [])
        thought_pattern = re.compile(r"^(.*?)\n\n\*\*Final Answer\*\*\n\n", re.DOTALL)
        output_pattern = re.compile(r"\n\n\*\*Final Answer\*\*\n\n(.*?)$", re.DOTALL)

        for message in messages:
            content = message.get("content", "")
            if "\n\n**Final Answer**\n\n" in content:
                thought_match = thought_pattern.search(content)
                thought = thought_match.group(1).strip()
                # Format the Thought block with a blockquote in Markdown
                thought = thought.replace("\n", "\n> ")
                output_match = output_pattern.search(content)
                output = output_match.group(1).strip()
                # Format the model's response with the revised Thought block, an "Output" header, and the Final Answer
                content = f"\n\n#### Thought:\n> {thought}\n\n#### Output:\n{output}\n"
                message["content"] = content

        body["messages"] = messages
        return body
```

ä¸Šè¿°å‡½æ•°èƒ½è®©æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ä¼˜é›…çš„æ˜¾ç¤ºå‡ºæ¥ï¼Œä¸‹é¢çš„å‡½æ•°åˆ™å¯ä»¥è®¡ç®—è¾“å…¥è¾“å‡ºçš„tokenæ•°é‡å’Œè¾“å‡ºé€Ÿåº¦ï¼ˆå¯é€‰ï¼‰ï¼Œæ–°å»ºä¸€ä¸ªæ–°çš„å‡½æ•°ï¼š
```python
"""
title: Time Token Tracker
author: owndev
author_url: https://github.com/owndev
project_url: https://github.com/owndev/Open-WebUI-Functions
funding_url: https://github.com/owndev/Open-WebUI-Functions
version: 2.3.0
license: MIT
description: A filter for tracking the response time and token usage of a request.
features:
  - Tracks the response time of a request.
  - Tracks Token Usage.
  - Calculates the average tokens per message.
  - Calculates the tokens per second.
"""

import time
from typing import Optional
import tiktoken
from pydantic import BaseModel, Field

# Global variables to track start time and token counts
global start_time, request_token_count, response_token_count


class Filter:
    class Valves(BaseModel):
        priority: int = Field(
            default=0, description="Priority level for the filter operations."
        )
        CALCULATE_ALL_MESSAGES: bool = Field(
            default=True,
            description="If true, calculate tokens for all messages. If false, only use the last user and assistant messages.",
        )
        SHOW_AVERAGE_TOKENS: bool = Field(
            default=True,
            description="Show average tokens per message (only used if CALCULATE_ALL_MESSAGES is true).",
        )
        SHOW_RESPONSE_TIME: bool = Field(
            default=True, description="Show the response time."
        )
        SHOW_TOKEN_COUNT: bool = Field(
            default=True, description="Show the token count."
        )
        SHOW_TOKENS_PER_SECOND: bool = Field(
            default=True, description="Show tokens per second for the response."
        )

    def __init__(self):
        self.name = "Time Token Tracker"
        self.valves = self.Valves()

    async def inlet(
        self, body: dict, __user__: Optional[dict] = None, __event_emitter__=None
    ) -> dict:
        global start_time, request_token_count
        start_time = time.time()

        model = body.get("model", "default-model")
        all_messages = body.get("messages", [])

        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        # If CALCULATE_ALL_MESSAGES is true, use all "user" and "system" messages
        if self.valves.CALCULATE_ALL_MESSAGES:
            request_messages = [
                m for m in all_messages if m.get("role") in ("user", "system")
            ]
        else:
            # If CALCULATE_ALL_MESSAGES is false and there are exactly two messages
            # (one user and one system), sum them both.
            request_user_system = [
                m for m in all_messages if m.get("role") in ("user", "system")
            ]
            if len(request_user_system) == 2:
                request_messages = request_user_system
            else:
                # Otherwise, take only the last "user" or "system" message if any
                reversed_messages = list(reversed(all_messages))
                last_user_system = next(
                    (
                        m
                        for m in reversed_messages
                        if m.get("role") in ("user", "system")
                    ),
                    None,
                )
                request_messages = [last_user_system] if last_user_system else []

        request_token_count = sum(
            len(encoding.encode(m["content"])) for m in request_messages
        )

        return body

    async def outlet(
        self, body: dict, __user__: Optional[dict] = None, __event_emitter__=None
    ) -> dict:
        global start_time, request_token_count, response_token_count
        end_time = time.time()
        response_time = end_time - start_time

        model = body.get("model", "default-model")
        all_messages = body.get("messages", [])

        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        reversed_messages = list(reversed(all_messages))

        # If CALCULATE_ALL_MESSAGES is true, use all "assistant" messages
        if self.valves.CALCULATE_ALL_MESSAGES:
            assistant_messages = [
                m for m in all_messages if m.get("role") == "assistant"
            ]
        else:
            # Take only the last "assistant" message if any
            last_assistant = next(
                (m for m in reversed_messages if m.get("role") == "assistant"), None
            )
            assistant_messages = [last_assistant] if last_assistant else []

        response_token_count = sum(
            len(encoding.encode(m["content"])) for m in assistant_messages
        )

        # Calculate tokens per second (only for the last assistant response)
        if self.valves.SHOW_TOKENS_PER_SECOND:
            last_assistant_msg = next(
                (m for m in reversed_messages if m.get("role") == "assistant"), None
            )
            last_assistant_tokens = (
                len(encoding.encode(last_assistant_msg["content"]))
                if last_assistant_msg
                else 0
            )
            resp_tokens_per_sec = (
                0 if response_time == 0 else last_assistant_tokens / response_time
            )

        # Calculate averages only if CALCULATE_ALL_MESSAGES is true
        avg_request_tokens = avg_response_tokens = 0
        if self.valves.SHOW_AVERAGE_TOKENS and self.valves.CALCULATE_ALL_MESSAGES:
            req_count = len(
                [m for m in all_messages if m.get("role") in ("user", "system")]
            )
            resp_count = len([m for m in all_messages if m.get("role") == "assistant"])
            avg_request_tokens = request_token_count / req_count if req_count else 0
            avg_response_tokens = response_token_count / resp_count if resp_count else 0

        # Shorter style, e.g.: "10.90s | Req: 175 (Ã˜ 87.50) | Resp: 439 (Ã˜ 219.50) | 40.18 T/s"
        description_parts = []
        if self.valves.SHOW_RESPONSE_TIME:
            description_parts.append(f"{response_time:.2f}s")
        if self.valves.SHOW_TOKEN_COUNT:
            if self.valves.SHOW_AVERAGE_TOKENS and self.valves.CALCULATE_ALL_MESSAGES:
                # Add averages (Ã˜) into short output
                short_str = (
                    f"è¾“å…¥: {request_token_count} (Ã˜ {avg_request_tokens:.2f}) | "
                    f"è¾“å‡º: {response_token_count} (Ã˜ {avg_response_tokens:.2f})"
                )
            else:
                short_str = (
                    f"è¾“å…¥: {request_token_count} | è¾“å‡º: {response_token_count}"
                )
            description_parts.append(short_str)
        if self.valves.SHOW_TOKENS_PER_SECOND:
            description_parts.append(f"{resp_tokens_per_sec:.2f} T/s")
        description = " | ".join(description_parts)

        await __event_emitter__(
            {
                "type": "status",
                "data": {"description": description, "done": True},
            }
        )
        return body

```

ä¿å­˜è¿™ä¸¤ä¸ªå‡½æ•°å¹¶å¯ç”¨ä»–ä»¬ï¼Œå¹¶ä¸”åœ¨å‡½æ•°è®¾ç½®ä¸­å¯ç”¨`å…¨å±€`ã€‚
ç°åœ¨è½¬åˆ°`è®¾ç½®`é€‰é¡¹å¡ï¼Œå¹¶è¿›å…¥`æ¨¡å‹`è®¾ç½®ï¼Œç‚¹å‡»`QwQ-32B`æ¨¡å‹å¹¶å¯ç”¨åˆšåˆšè®¾ç½®å¥½çš„å‡½æ•°ã€‚
![[Pasted image 20250308144143.png]]

ï¼ˆæ¨¡å‹çš„`icon`éœ€è¦è‡ªå·±è®¾ç½®ï¼Œç›´æ¥ä»HFçš„ç½‘é¡µä¸Šæ‰’ä¸‹æ¥å°±å¯ä»¥äº†ğŸ¸ï¼‰

ä¸ºäº†è®©é™¤ç®¡ç†å‘˜ä»¥å¤–çš„ç”¨æˆ·è®¿é—®åˆ°ï¼Œè¿˜éœ€è¦è®¾ç½®ç”¨æˆ·ç»„æƒé™ã€‚
![[Pasted image 20250308144313.png]]

åˆ°è¿™é‡Œï¼Œæ‰€éœ€è¦çš„è®¾ç½®å·²ç»å…¨éƒ¨æå®šï¼